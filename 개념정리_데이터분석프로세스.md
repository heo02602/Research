## 데이터 분석 업무 프로세스

### 1. 문제 정의
- 데이터 분석의 대상과 목적, 목표는 무엇인가를 명확하게 정의해야 한다.
- 정의된 문제를 해결하기 위한 구체적인 계획을 수립해야 한다.
- 문제를 정의할 분야에 대한 비즈니스 지식이 필요하다.
- 모든 사람들이 명료하게 이해할 수 있도록 구체적이어야 한다.

### 2. 데이터 수집
- 생각할 수 있는 모든 관련 데이터에 대해서 모두 정의하고 조사하는 것이 가장 중요하다.
- 피쉬본 다이어그램을 이용하면 누락 없이 조사할 수 있다.
- 분석에 필요한 데이터를 찾고 모으는 단계이다.
- 데이터 엔지니어의 역할이 큰 비중을 차지한다.
- 수집 방법 : 데이터 구매, 실험 환경에서 수집(스스로 데이터 생성), 웹 크롤링, 오픈 데이터셋, 오픈 API, RSS, Streaming, Aggregator(Log, RDB 등), ETL 등

### 3. 데이터 적재 및 저장
- 수집한 데이터를 분산 스토리지에 영구 또는 임시로 적재하는 기술이다.
- 수집된 데이터의 성격에 따라 분산 저장소를 선택적으로 사용할 수 있는 아키텍처링이 이뤄져야 한다.

### 4. 데이터 가공 및 정제
- 다음 단계를 위해 비정형 데이터(음성,이미지,텍스트,동영상 등)를 정형 데이터(스키마가 있는 구조)로 가공하거나, 개인정보로 의심되는 데이터를 비식별화 처리하는 작업이 필요하다.

### 5. 데이터 전처리 (데이터 가공 및 정제)
- 가장 많은 노력과 고생이 필요한 단계이다.
- 적재된 데이터를 분석에 활용하기 위해 데이터를 정형화 및 정규화하는 기술이다.
- 데이터를 생성할 때 분석을 전제로 데이터를 생성하지 않은 경우가 대부분이므로, 전처리 과정이 필요하다.
- 분석에 부적합한 구조, 누락된 항목, NA(결측값) 존재 등으로 인해 전처리 과정이 필요하다.
- 노이즈 제거, 중복값 제거, 결측값 보정, 데이터 연계/통합, 데이터 구조 변경(차원 변경)
- 데이터 벡터화, Outlier Detection(Anomaly Detection), Feature Engineering 등
- 데이터의 결측값, 이상치, 중복값 등을 처리하여 품질이 좋은 데이터로 통합한다.
- 수집된 데이터를 정리하고 표준화하며 통합하는 일련의 과정이다.
- 데이터를 분석하기 전에 분석에 적합한 데이터를 만드는 사전 처리 전반을 일컫는다.
- 데이터를 활용한 산출물을 만들려면 원천 데이터 확보와 데이터 가공이 필수적으로 선행되어야 한다.
- 데이터 분석에서도 의미 있는 분석 결과를 도출하려면 데이터에 내재한 여러 오류를 먼저 찾고(정제), 동일한 형태로 통일(표준화) 시키는 작업을 수행해야 한다.
- 데이터를 유의미하게 활용하려면 원래 데이터를 활용 목적에 맞게 변경해야 한다.
- '쓸 만한 데이터가 부족하다'란 말은 '활용 목적에 맞게 데이터를 가공할 기술과 능력이 부족하다'는 뜻이다. 데이터 가공 기술 및 능력을 갖추면 쓸 만한 데이터가 늘어날 수 밖에 없다.
- 데이터를 원하는 형태로 적절히 가공하려면 해당 분야별 전문가나 전문 지식 및 기술이 필요하다.
- 정형 데이터를 분석하는 분야에서는 분석을 위한 데이터 세트 구축시 인간의 수작업이 굳이 필요하지 않다.
- 비정형 데이터를 다루는 인공지능 분야에서는 인간 노동력의 투자가 선행되어야 한다.
- 인간 노동력이 가급적 최소화되어야 한다. 여기서 문제는 시간과 비용이다.
- 성능과 비용을 고려하여 타협점을 찾아야 하는데, 그 중 하나가 바로 '데이터 가공의 자동화'이다.
- 라벨링된 자료가 충분히 축적되면 가공 및 정제 규칙을 알고리즘화하여 새로 수집된 데이터를 자동으로 어노테이션하고 라벨링할 수 있다.

### 6. 데이터 탐색
- 가장 핵심적인 부분 중 하나이다.
- 데이터의 특성을 파악하고 여러 관계를 찾는 단계이다.
- 데이터가 어떤 특징을 가지고 있는지 파악하면 모델링에 대한 많은 생각을 얻을 수 있다.
- EDA (탐색적 데이터 분석) 등 많은 방법이 있다.
- EDA 과정에서는 분석 대상 데이터를 탐색함으로써 데이터의 특징을 이해하고 파악하는 과정으로서 주로 변수 파악, 통계량 산출, 상관 분석 등을 수행하며, 시각화를 통해 데이터에 대한 인사이트를 얻는다.

### 7. 모델링
- 원하는 결과를 도출하기 위해서 예측, 분류, 회귀 등을 위한 작업을 진행하는 단계이다.
- 전처리된 데이터를 관점별로 나누고 쪼개어 본다.
- 성능을 높이기 위해 파라미터(가중치) 튜닝 작업을 진행하게 된다.
- 많은 비용이 발생할 수 있다.
- 머신러닝, 딥러닝, 통계 기법 등이 사용된다.

> 기초 통계 분석 과정
  - 데이터 평균, 표준편차와 같은 대푯값과 데이터 간의 상관 계수 등을 계산하여 각 데이터의 특성을 파악하는 과정이다.
> 모델 구축 및 평가 과정
  - 모델이란 각 알고리즘이 데이터 분석을 진행하면서 생성하는 로직이나 수식을 말한다.
  - 통계적 알고리즘, 인공지능으로 대표되는 머신러닝/딥러닝, 데이터 마이닝 분야의 알고리즘을 사용하고 모델을 구현한다
  - 수집한 데이터를 Training Data와 Test Data로 분할한다. (일반적으로 7:3)
  - Training Data로 여러 가지 알고리즘을 사용하여 모델을 구현한다.
  - Test Data로 모델의 성능을 평가한다.
  - 평가 결과를 바탕으로 다시 데이터 분석을 하며 최적화된 알고리즘과 모델을 선택한다.

### 8. 검증 및 고찰
- 모델링을 통해 결과가 도출되면 이를 처음 정의했던 문제와 연관시켜 문제를 해결하는 방법을 모색하는 단계이다.
- 분석 보고서로서, 데이터를 분석해서 얻은 새로운 정보와 앞으로의 방향 등을 나타낸다.
- 회귀식과 같은 모델로 주로 기업 등의 생산 데이터나 품질 데이터를 분석했을 때 얻을 수 있는 결과이며, 생산성이나 품질 향상을 기대할 수 있다.

### 9. 해석 및 시각화
- 시각화나 레포팅을 통해 도출된 결과를 알아보기 쉽게 표현하고, 이를 근거로 활용할 수 있다.
